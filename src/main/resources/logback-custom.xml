<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!--配置获取配置中心的信息-->
    <springProperty scope="context" name="kafkaAddress" source="log.kafka.address" defaultValue="info"/>
    <springProperty scope="context" name="kafkaTopic" source="log.kafka.topic" defaultValue="info"/>
    <!--配置获取ip规则-->
    <conversionRule conversionWord="ip" converterClass="com.example.springCloud.config.IPConverterConfig"/>
    <!-- 日志存放路径 -->
    <property name="log.path" value="/Applications/work/local/data/logs"/>
    <!-- 日志输出格式 -->
    <property name="log.pattern"
              value="%d{yyyy-MM-dd HH:mm:ss.SSS}-[%X{requestId}] -[%ip]- [%thread] %-5level %logger{20} - [%method,%line] - %msg%n"/>

    <!-- 控制台输出 -->
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>${log.pattern}</pattern>
        </encoder>
    </appender>

    <!-- 系统日志输出 -->
    <appender name="file_info" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${log.path}/spring-cloud-info.log</file>
        <!-- 循环政策：基于时间创建日志文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 日志文件名格式 -->
            <fileNamePattern>${log.path}/spring-cloud-info.%d{yyyy-MM-dd}.log</fileNamePattern>
            <!-- 日志最大的历史 60天 -->
            <maxHistory>1</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>${log.pattern}</pattern>
        </encoder>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- 过滤的级别 -->
            <level>INFO</level>
            <!-- 匹配时的操作：接收（记录） -->
            <onMatch>ACCEPT</onMatch>
            <!-- 不匹配时的操作：拒绝（不记录） -->
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <appender name="file_error" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${log.path}/spring-cloud-error.log</file>
        <!-- 循环政策：基于时间创建日志文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 日志文件名格式 -->
            <fileNamePattern>${log.path}/spring-cloud-error.%d{yyyy-MM-dd}.log</fileNamePattern>
            <!-- 日志最大的历史 60天 -->
            <maxHistory>1</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>${log.pattern}</pattern>
        </encoder>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- 过滤的级别 -->
            <level>ERROR</level>
            <!-- 匹配时的操作：接收（记录） -->
            <onMatch>ACCEPT</onMatch>
            <!-- 不匹配时的操作：拒绝（不记录） -->
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}-[%X{requestId}]-[%ip] - [%thread] %-5level %logger{20} -
                [%method,%line] -
                %msg%n
            </pattern>
        </encoder>
        <topic>${kafkaTopic}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!--        <producerConfig>bootstrap.servers=127.0.0.1:9092</producerConfig>-->
        <producerConfig>bootstrap.servers=${kafkaAddress}</producerConfig>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- 过滤的级别 -->
            <level>INFO</level>
            <!-- 匹配时的操作：接收（记录） -->
            <onMatch>ACCEPT</onMatch>
            <!-- 不匹配时的操作：拒绝（不记录） -->
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>


    <appender name="kafkaAppenderError" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}-[%X{requestId}]-[%ip] - [%thread] %-5level %logger{20} -
                [%method,%line] -
                %msg%n
            </pattern>
        </encoder>
        <topic>test3</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!--        <producerConfig>bootstrap.servers=127.0.0.1:9092</producerConfig>-->
        <producerConfig>bootstrap.servers=${kafkaAddress}</producerConfig>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <!-- 过滤的级别 -->
            <level>ERROR</level>
            <!-- 匹配时的操作：接收（记录） -->
            <onMatch>ACCEPT</onMatch>
            <!-- 不匹配时的操作：拒绝（不记录） -->
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>
    <!--   需要再看看异步接入kafka-->
    <appender name="KAFKAASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <!-- 默认如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志，若要保留全部日志，设置为0 -->
        <!--        <discardingThreshold>30</discardingThreshold>-->
        <queueSize>1024</queueSize>
        <appender-ref ref="kafkaAppender"/>
    </appender>

    <appender name="KAFKAASYNCERROR" class="ch.qos.logback.classic.AsyncAppender">
        <!-- 默认如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志，若要保留全部日志，设置为0 -->
        <!--        <discardingThreshold>30</discardingThreshold>-->
        <queueSize>1024</queueSize>
        <appender-ref ref="kafkaAppenderError"/>
    </appender>
    <!-- Spring日志级别控制  -->
    <logger name="org.springframework" level="warn"/>

    <root level="info">
        <appender-ref ref="console"/>
    </root>

    <!--系统操作日志-->
    <root level="info">
        <appender-ref ref="file_info"/>
        <appender-ref ref="file_error"/>
        <!--        <appender-ref ref="KAFKAASYNC"/>-->
        <!--        <appender-ref ref="KAFKAASYNCERROR"/>-->
    </root>
</configuration>